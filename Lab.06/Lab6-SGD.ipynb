{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup - Ridge Regression \n",
    "Cost function: $$\\begin{align} J(\\theta) &= \\frac{1}{m}\\sum_{i=1}^m(\\theta^Tx_i- y_i)^2 + \\lambda  \\theta^T\\theta, \\; x_i, \\theta \\in \\mathrm{R}^{d+1} \\\\\n",
    "&= \\frac{1}{m}[(X\\theta-y)^{T} (X\\theta-y)] + \\lambda \\theta^T\\theta, \\; X \\in \\mathrm{R}^{m\\times(d+1)}\n",
    "\\end{align}$$\n",
    "Gradient of cost function: $$\\begin{align} \n",
    "\\nabla J(\\theta) = \\frac{1}{m}[2X^{T}X\\theta-2X^{T}y]+2\\lambda\\theta\n",
    "\\end{align}$$\n",
    "When $m$ (number of data points) is large, computing a single gradient can take a very long time! \n",
    "\n",
    "### Stochastic Gradient Descent \n",
    "\n",
    "When the cost function takes in the form of $J(\\theta) = \\frac{1}{m}\\sum_{i=1}^mf_i(\\theta)$, we can show that $\\nabla f_i(\\theta)$ for some data point $i$ chosen uniformly at random from the data set is an unbiased estimator of $\\nabla J(\\theta)$. Instead of taking a gradient step of $-\\nabla J(\\theta)$, we instead take $-\\nabla f_i(\\theta)$, this is called **Stochastic Gradient Descent**\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^mf_i(\\theta), \\text{ where } f_i(\\theta) = (\\theta^Tx_i- y_i)^2 + \\lambda  \\theta^T\\theta$$\n",
    "$$\\nabla f_{i}(\\theta) = 2x_i^{T}(x_i\\theta-y_i)+2\\lambda\\theta $$\n",
    "$$E(\\nabla f_{i}(\\theta)) = \\frac{1}{m}\\sum_{i=1}^m(\\nabla f_{i}(\\theta)) = \\nabla J(\\theta)$$\n",
    "\n",
    "parameter update rule: \n",
    "$$\\theta \\leftarrow \\theta - \\eta ( 2x_i^{T}(x_i\\theta-y_i)+2\\lambda\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Compute the gradient of L2-regularized square loss function given X, y and theta\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "        lambda_reg - the regularization coefficient\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    First_term = np.dot(np.dot(X.T,X),theta)\n",
    "    Second_term = (X.T).dot(y)\n",
    "    num_instances = X.shape[0]\n",
    "    grad = (First_term-Second_term)*2/num_instances + theta*2*lambda_reg\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_grad_descent(X, y, alpha=0.1, lambda_reg=1, num_iter=1000):\n",
    "    \"\"\"\n",
    "    In this question you will implement stochastic gradient descent with a regularization term\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - string or float. step size in gradient descent\n",
    "                NOTE: In SGD, it's not always a good idea to use a fixed step size. Usually it's set to 1/sqrt(t) or 1/t\n",
    "                if alpha is a float, then the step size in every iteration is alpha.\n",
    "                if alpha == \"1/sqrt(t)\", alpha = 1/sqrt(t)\n",
    "                if alpha == \"1/t\", alpha = 1/t\n",
    "        lambda_reg - the regularization coefficient\n",
    "        num_iter - number of epochs (i.e number of times to go through the whole training set)\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 3D numpy array of size (num_iter, num_instances, num_features)\n",
    "        loss hist - the history of **regularized loss function** vector, 2D numpy array of size(num_iter, num_instances)\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta = np.ones(num_features) #Initialize theta\n",
    "\n",
    "    theta_hist = np.zeros((num_iter, num_instances, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros((num_iter, num_instances)) #Initialize loss_hist\n",
    "\n",
    "    t = 1\n",
    "    arr = np.arange(num_instances)\n",
    "    np.random.shuffle(arr)\n",
    "    for i in range(num_iter): #each epoch\n",
    "        np.random.shuffle(arr) #shuffle once every epoch\n",
    "        for j in arr:\n",
    "            grad = compute_regularized_square_loss_gradient(X[j:j+1,:], y[j:j+1], theta, lambda_reg)\n",
    "            if type(alpha) == float:\n",
    "                alpha_new = alpha\n",
    "            elif alpha == \"1/t\":\n",
    "                alpha_new = 1/(t+10)\n",
    "            elif alpha == \"1/sqrt(t)\":\n",
    "                alpha_new = 1/((t+10)**0.5)\n",
    "            theta = theta - alpha_new * grad\n",
    "            loss = compute_square_loss(X,y,theta)\n",
    "            \n",
    "            theta_hist[i,j] = theta\n",
    "            loss_hist[i,j] = loss\n",
    "            t+=1\n",
    "\n",
    "    return theta_hist, loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD in Sklearn: https://scikit-learn.org/stable/modules/sgd.html#id1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

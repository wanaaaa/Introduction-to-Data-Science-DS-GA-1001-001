{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. build probability distribution over all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in document\n",
    "doc = open('big.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out punctuations, leaving only vocabulary\n",
    "def tokenize(text): \n",
    "    return re.findall(r'\\w+', text.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Bali in March, Egypt in November: 12 Months of Travel Deals'\n",
    "tokenized_sent = tokenize(sent)\n",
    "\n",
    "#print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115585"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize the entire document\n",
    "tokenized_doc = tokenize(doc)\n",
    "len(tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'bali': 1,\n",
       "         'in': 2,\n",
       "         'march': 1,\n",
       "         'egypt': 1,\n",
       "         'november': 1,\n",
       "         '12': 1,\n",
       "         'months': 1,\n",
       "         'of': 1,\n",
       "         'travel': 1,\n",
       "         'deals': 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use Counter to count how many times each vocabulary appears in the document\n",
    "Counter(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 79809),\n",
       " ('of', 40024),\n",
       " ('and', 38312),\n",
       " ('to', 28765),\n",
       " ('in', 22023),\n",
       " ('a', 21124),\n",
       " ('that', 12512),\n",
       " ('he', 12401),\n",
       " ('was', 11410),\n",
       " ('it', 10681)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_COUNT = Counter(tokenized_doc)\n",
    "VOCAB_COUNT.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79809"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_COUNT.get('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate possible candidates of correction for a misspelled word\n",
    "1. Edit distance 1:\n",
    "    1. **delete a letter**; e.g. coup -> cou/cup/oup/cop\n",
    "    2. **insert a letter**; e.g. coup -> acoup/bcoup/..../caoup/cboup/..../coaup/cobup/....\n",
    "    3. **replace a letter**; e.g. coup -> corp/...\n",
    "    4. **swap 2 adjacent letters**; e.g. coup -> ocup/cuop/....\n",
    "\n",
    "2. Edit distance 2: all combinations that are within edit distance 1 of all the words within edit distance 1 of the original word\n",
    "\n",
    "    --> A lot of words! but not every one of them makes sense <br>\n",
    "    --> Filter out those that does not appear in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the set of candidate words that are one edit away.  One way is to *split* the original word in all possible places, each split forming a *pair* of words, `(a, b)`, before and after the place, and at each place, either delete, transpose, replace, or insert a letter:\n",
    "\n",
    "<table>\n",
    "  <tr><td> pairs: <td><tt> Ø+wird <td><tt> w+ird <td><tt> wi+rd <td><tt>wir+d<td><tt>wird+Ø<td><i>Notes:</i><tt> (<i>a</i>, <i>b</i>)</tt> pair</i>\n",
    "  <tr><td> deletions: <td><tt>Ø+ird<td><tt> w+rd<td><tt> wi+d<td><tt> wir+Ø<td><td><i>Delete first char of b</i>\n",
    "  <tr><td> transpositions: <td><tt>Ø+iwrd<td><tt> w+rid<td><tt> wi+dr</tt><td><td><td><i>Swap first two chars of b\n",
    "  <tr><td> replacements: <td><tt>Ø+?ird<td><tt> w+?rd<td><tt> wi+?d<td><tt> wir+?</tt><td><td><i>Replace char at start of b\n",
    "  <tr><td> insertions: <td><tt>Ø+?+wird<td><tt> w+?+ird<td><tt> wi+?+rd<td><tt> wir+?+d<td><tt> wird+?+Ø</tt><td><i>Insert char between a and b\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'wird'), ('w', 'ird'), ('wi', 'rd'), ('wir', 'd'), ('wird', '')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def splits(word):\n",
    "    return [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "\n",
    "splits('wird')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [L + R[1:]               for L, R in pairs if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in pairs if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in pairs if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in pairs for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(edits1('class'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words): \n",
    "    \"The subset of 'words' that exist in the document\"\n",
    "    return set(w for w in words if w in VOCAB_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#known(edits1('class'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Correct misspelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=VOCAB_COUNT.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spelling',\n",
       " 'errors',\n",
       " 'in',\n",
       " 'something',\n",
       " 'whatever',\n",
       " 'unusual',\n",
       " 'mistakes',\n",
       " 'everywhere']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(correction, tokenize('Speling errurs in somethink. Whutever; unusuel misteakes everyware?')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leno13win10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\leno13win10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\leno13win10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# the following codes only need to run once\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to lower case\n",
    "\n",
    "sent = 'Bali in M;arch, Egypt in November: 12 Months of Travel Deals'\n",
    "sent = sent.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove numbers and punctuations\n",
    "import re\n",
    "\n",
    "## remove punctuation\n",
    "sent = re.sub(r'[^\\w\\s]','',sent)\n",
    "# print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove digits\n",
    "sent = re.sub(r'\\d+','', sent)\n",
    "# print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "sent = sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sent = [s for s in sent if not s in stopwords.words('english')]\n",
    "\n",
    "# print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "strings = ['likes', 'liking', 'liked']\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# for s in strings:\n",
    "#     print(stemmer.stem(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference and Resources\n",
    "1. [Regex Cheat Sheet 1](https://regexr.com/), [Regex Cheat Sheet 2](https://www.rexegg.com/regex-quickstart.html)\n",
    "2. [More on spell check](https://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb)\n",
    "3. [Data Source](http://norvig.com/big.txt)\n",
    "4. https://norvig.com/spell-correct.html\n",
    "5. https://www.kaggle.com/shashanksai/text-preprocessing-using-python\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
